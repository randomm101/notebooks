{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "_KqmrDKOHX2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/"
      ],
      "metadata": {
        "id": "eTg_IB9RsRPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q timm pytorch_lightning"
      ],
      "metadata": {
        "id": "U3dlslrfHdZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "import gdown\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torchvision import transforms\n",
        "from timm import create_model\n",
        "import pytorch_lightning as pl\n",
        "from torchmetrics import Metric, MetricCollection\n",
        "from torchmetrics.classification import BinaryAccuracy\n",
        "\n",
        "\n",
        "pl.seed_everything(42)\n"
      ],
      "metadata": {
        "id": "npRTI0VimoWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f015bdc8-2b7b-4abb-fddb-f7021bceca0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(\"logs/\"):\n",
        "    !rm -r logs/"
      ],
      "metadata": {
        "id": "E5AeSY4gpiln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DTYPE = torch.int\n",
        "\n",
        "def transform_data():\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.CenterCrop(178),\n",
        "            transforms.Resize(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        ]\n",
        "    )\n",
        "    return transform\n",
        "\n",
        "class CelebADataset(Dataset):\n",
        "    def __init__(\n",
        "        self, root_dir: str, spurious_label: str, split: int, transforms=None\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.spurious_label = spurious_label\n",
        "        if transforms is None:\n",
        "            self.transforms = transform_data()\n",
        "        else:\n",
        "            self.transforms = transforms\n",
        "\n",
        "        self.metadata = pd.read_csv(\n",
        "            os.path.join(root_dir, \"list_attr_celeba.csv\"),\n",
        "            delim_whitespace=True,\n",
        "            header=1,\n",
        "            index_col=0,\n",
        "        )\n",
        "        for c in self.metadata.columns:\n",
        "            self.metadata[c] = self.metadata[c].apply(lambda x: 0 if x == -1 else 1)\n",
        "\n",
        "        df_partitions = pd.read_csv(\n",
        "            os.path.join(root_dir, \"list_eval_partition.csv\"),\n",
        "            delim_whitespace=True,\n",
        "            header=None,\n",
        "            index_col=0,\n",
        "        )\n",
        "        indices = df_partitions[df_partitions.iloc[:, 0] == split].index\n",
        "\n",
        "        self.metadata = self.metadata[[\"Blond_Hair\", spurious_label]].loc[indices, :]\n",
        "        self.metadata.rename(\n",
        "            columns={\"Blond_Hair\": \"label\", spurious_label: \"spurious_label\"},\n",
        "            inplace=True,\n",
        "        )\n",
        "        self.metadata[\"group\"] = self.metadata.apply(\n",
        "            lambda x: 2 * x[\"label\"] + x[\"spurious_label\"], axis=1\n",
        "        )\n",
        "\n",
        "        self.groups = torch.as_tensor(self.metadata[\"group\"].values, dtype=DTYPE)\n",
        "        self.group_counts = (\n",
        "            torch.arange(4, dtype=torch.int).unsqueeze(1).eq(self.groups).sum(dim=1)\n",
        "        )\n",
        "        self.labels = torch.as_tensor(self.metadata[\"label\"].values, dtype=DTYPE)\n",
        "        self.label_counts = (\n",
        "            torch.arange(2, dtype=torch.int).unsqueeze(1).eq(self.labels).sum(dim=1)\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample = self.metadata.iloc[index, :]\n",
        "\n",
        "        filename = os.path.join(self.root_dir, \"img_align_celeba/\", sample.name)\n",
        "        image = Image.open(filename).convert(\"RGB\")\n",
        "        image = self.transforms(image)\n",
        "\n",
        "        items = {}\n",
        "        items[\"filename\"] = sample.name\n",
        "        items[\"image\"] = image\n",
        "        items[\"label\"] = torch.as_tensor(sample[\"label\"], dtype=DTYPE)\n",
        "        items[\"spurious_label\"] = torch.as_tensor(sample[\"spurious_label\"], dtype=DTYPE)\n",
        "        items[\"group\"] = torch.as_tensor(sample[\"group\"], dtype=DTYPE)\n",
        "        return items\n"
      ],
      "metadata": {
        "id": "BfFvEogytOaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stratified_sampler(dataset: Dataset):\n",
        "    weights = len(dataset) / dataset.label_counts\n",
        "    weights = weights[dataset.labels.long()]\n",
        "\n",
        "    sampler = WeightedRandomSampler(\n",
        "        weights,\n",
        "        len(dataset),\n",
        "        replacement=True,\n",
        "    )\n",
        "    return sampler\n"
      ],
      "metadata": {
        "id": "nclc4J2UsvbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CelebADataModule(pl.LightningDataModule):\n",
        "    def __init__(self, root_dir: str, spurious_label: str, stratified_sampling, transforms, batch_size, num_workers):\n",
        "        super().__init__()\n",
        "\n",
        "        self.root_dir = root_dir\n",
        "        self.spurious_label = spurious_label\n",
        "        if transforms is None:\n",
        "            self.transforms = transform_data\n",
        "        else:\n",
        "            self.transforms = transforms\n",
        "        self.stratified_sampling = stratified_sampling\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "\n",
        "    def prepare_data(self):\n",
        "        files = {\n",
        "            \"list_eval_partition.csv\": \"1kDqtHZHpYMe7rt1zu9pOevzUbApkDNRa\",\n",
        "            \"list_attr_celeba.csv\": \"1s8CyrddcxHdvwro-_M25H7uxsDWL_1Bs\",\n",
        "            \"img_align_celeba.zip\": \"1mGM-w9373aW5UJ27xa5oAsesL06JOe3h\",\n",
        "        }\n",
        "        os.makedirs(self.root_dir, exist_ok=True)\n",
        "        for file, file_id in files.items():\n",
        "            print(f\"Downloading {file}...\")\n",
        "            path = os.path.join(self.root_dir, file)\n",
        "            if not os.path.exists(path):\n",
        "                gdown.download(id=file_id, output=path, quiet=True)\n",
        "            else:\n",
        "                print(\"This file already exists.\")\n",
        "\n",
        "            if (\n",
        "                file.endswith(\"zip\") and\n",
        "                not os.path.exists(os.path.join(self.root_dir, file.replace(\".zip\", \"\")))\n",
        "            ):\n",
        "                print(f\"Unzipping {file}...\")\n",
        "                with zipfile.ZipFile(path, \"r\") as zip_ref:\n",
        "                    zip_ref.extractall(self.root_dir)\n",
        "\n",
        "\n",
        "    def setup(self, stage: str = None):\n",
        "        self.train_dataset = CelebADataset(self.root_dir, self.spurious_label, 0)\n",
        "        self.val_dataset = CelebADataset(self.root_dir, self.spurious_label, 1)\n",
        "        # self.test_dataset = CelebADataset(self.root_dir, self.spurious_label, 2)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        shuffle = False\n",
        "        sampler = None\n",
        "        if self.stratified_sampling:\n",
        "            sampler = stratified_sampler(self.train_dataset)\n",
        "        else:\n",
        "            shuffle = True\n",
        "\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            # prefetch_factor=2 * (self.batch_size // self.num_workers),\n",
        "            pin_memory=True,\n",
        "            shuffle=shuffle,\n",
        "            sampler=sampler,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            # prefetch_factor=2 * (self.batch_size // self.num_workers),\n",
        "            pin_memory=True,\n",
        "            shuffle=False,\n",
        "        )"
      ],
      "metadata": {
        "id": "5OINXMOhTbyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WorstGroupAccuracy(Metric):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.add_state(\"correct\", torch.zeros(4), dist_reduce_fx=\"sum\")\n",
        "        self.add_state(\"total\", torch.zeros(4), dist_reduce_fx=\"sum\")\n",
        "\n",
        "    def update(self, y_pred, y_true, g):\n",
        "        self.total += torch.arange(4, device=self.device).unsqueeze(1).eq(g).sum(dim=1)\n",
        "\n",
        "        is_correct = y_true == y_pred\n",
        "        for i in range(4):\n",
        "            indices = torch.nonzero(g == i).squeeze(1)\n",
        "            self.correct[i] += is_correct[indices].sum()\n",
        "\n",
        "    def compute(self):\n",
        "        x = self.correct.float() / self.total\n",
        "        x[x.isnan()] = 0.0\n",
        "        wg_acc = x.min()\n",
        "        return wg_acc, x"
      ],
      "metadata": {
        "id": "nogx-SNxo7ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hparams,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.save_hyperparameters()\n",
        "        self.model = create_model(**hparams[\"model\"])\n",
        "\n",
        "        self.optimizer_config = hparams[\"optimizer\"]\n",
        "\n",
        "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        self.train_accuracy = BinaryAccuracy()\n",
        "        self.valid_accuracy = BinaryAccuracy()\n",
        "        self.train_wga = WorstGroupAccuracy()\n",
        "        self.valid_wga = WorstGroupAccuracy()\n",
        "\n",
        "    def forward(self, batch, validation: bool = False):\n",
        "        batch.pop(\"filename\")\n",
        "        x = batch[\"image\"]\n",
        "        g = batch[\"group\"]\n",
        "        y_true = batch[\"label\"].float()\n",
        "        if y_true.ndim < 2:\n",
        "            y_true.unsqueeze_(1)\n",
        "\n",
        "        logits = self.model(x)\n",
        "        if logits.size(1) < 2:\n",
        "            y_pred = (torch.sigmoid(logits) >= 0.5).float()\n",
        "        else:\n",
        "            y_pred = torch.argmax(torch.softmax(logits, dim=1), dim=1).unsqueeze(1)\n",
        "\n",
        "        loss = self.loss_fn(logits, y_true)\n",
        "        self.log(f\"{self.logging_prefix}/loss\", loss, prog_bar=self.log_progress_bar, logger=True, sync_dist=True)\n",
        "\n",
        "        if validation:\n",
        "            self.valid_accuracy.update(y_pred, y_true)\n",
        "            self.valid_wga.update(y_pred, y_true, g)\n",
        "        else:\n",
        "            self.train_accuracy.update(y_pred, y_true)\n",
        "            self.train_wga.update(y_pred, y_true, g)\n",
        "\n",
        "            avg_acc = self.train_accuracy.compute(y_pred, y_true)\n",
        "            self.log(\n",
        "                f\"{self.logging_prefix}/step:avg_acc\",\n",
        "                avg_acc,\n",
        "                on_step=self.log_on_step,\n",
        "                on_epoch=self.log_on_epoch,\n",
        "                logger=True,\n",
        "            )\n",
        "\n",
        "            wga, acc_groups = self.train_wga.compute(y_pred, y_true, g)\n",
        "            self.log(f\"{self.logging_prefix}/step:wga\", wga, on_step=self.log_on_step, logger=True)\n",
        "            for k, v in enumerate(acc_groups):\n",
        "                self.log(f\"{self.logging_prefix}/step:acc_grp{k}\", v, on_step=self.log_on_step, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        self.logging_prefix = \"train\"\n",
        "        self.log_on_step = True\n",
        "        self.log_on_epoch = False\n",
        "        loss = self(batch)\n",
        "        self.log_on_step = None\n",
        "        self.log_on_epoch = None\n",
        "        self.logging_prefix = None\n",
        "        return loss\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        self.log(\n",
        "            \"val/epoch:avg_acc\",\n",
        "            self.train_accuracy.compute(),\n",
        "            prog_bar=True,\n",
        "            on_step=False,\n",
        "            on_epoch=True,\n",
        "            logger=True,\n",
        "        )\n",
        "\n",
        "        wga, acc_groups = self.train_wga.compute()\n",
        "        self.log(\n",
        "            \"train/epoch:wga\",\n",
        "            wga,\n",
        "            prog_bar=True,\n",
        "            on_step=False,\n",
        "            on_epoch=True,\n",
        "            logger=True,\n",
        "        )\n",
        "        for k, v in enumerate(acc_groups):\n",
        "            self.log(\n",
        "                f\"train/epoch:acc_grp{k}\",\n",
        "                v,\n",
        "                on_step=False,\n",
        "                on_epoch=True,\n",
        "                logger=True,\n",
        "            )\n",
        "\n",
        "        self.train_accuracy.reset()\n",
        "        self.train_wga.reset()\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        self.logging_prefix = \"val\"\n",
        "        self.log_progress_bar = True\n",
        "        loss = self(batch, validation=True)\n",
        "        self.logging_prefix = None\n",
        "        self.log_progress_bar = False\n",
        "        return loss\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        self.log(\n",
        "            \"val/epoch:avg_acc\",\n",
        "            self.valid_accuracy.compute(),\n",
        "            prog_bar=True,\n",
        "            on_step=False,\n",
        "            on_epoch=True,\n",
        "            logger=True,\n",
        "        )\n",
        "\n",
        "        wga, acc_groups = self.valid_wga.compute()\n",
        "        self.log(\n",
        "            \"val/epoch:wga\",\n",
        "            wga,\n",
        "            prog_bar=True,\n",
        "            on_step=False,\n",
        "            on_epoch=True,\n",
        "            logger=True,\n",
        "        )\n",
        "        for k, v in enumerate(acc_groups):\n",
        "            self.log(\n",
        "                f\"val/epoch:acc_grp{k}\",\n",
        "                v,\n",
        "                on_step=False,\n",
        "                on_epoch=True,\n",
        "                logger=True,\n",
        "            )\n",
        "\n",
        "        self.valid_accuracy.reset()\n",
        "        self.valid_wga.reset()\n",
        "\n",
        "    @property\n",
        "    def num_training_steps(self) -> int:\n",
        "        \"\"\"Get number of training steps\"\"\"\n",
        "        if self.trainer.max_steps > -1:\n",
        "            return self.trainer.max_steps\n",
        "\n",
        "        self.trainer.fit_loop.setup_data()\n",
        "        num_devices = max(1, self.trainer.num_devices)\n",
        "        dataset_size = len(self.trainer.train_dataloader)\n",
        "        num_steps = dataset_size * self.trainer.max_epochs // (self.trainer.accumulate_grad_batches * num_devices)\n",
        "        return num_steps\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer_name = self.optimizer_config.pop(\"optimizer_name\", \"AdamW\")\n",
        "        optimizer = getattr(torch.optim, optimizer_name)\n",
        "        optimizer = optimizer(self.model.parameters(), **self.optimizer_config)\n",
        "\n",
        "        t = [1] * self.num_training_steps\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, [lambda x: t.__getitem__(x)])\n",
        "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]\n"
      ],
      "metadata": {
        "id": "5Jzf48_1wDnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hparams = {\n",
        "    \"dataset\": {\n",
        "        \"root_dir\": \"celeba/\",\n",
        "        \"spurious_label\": \"Male\",\n",
        "        \"stratified_sampling\": True,\n",
        "        \"batch_size\": 128,\n",
        "        \"num_workers\": 2,\n",
        "        \"transforms\": None,\n",
        "    },\n",
        "    \"optimizer\": {\n",
        "        \"optimizer_name\": \"SGD\",\n",
        "        \"lr\": 1e-5,\n",
        "        \"weight_decay\": 1.0,\n",
        "        \"momentum\": 0.9,\n",
        "    },\n",
        "    \"model\": {\n",
        "        \"model_name\": \"resnet18\",\n",
        "        \"pretrained\": True,\n",
        "        \"num_classes\": 1,\n",
        "    },\n",
        "    \"max_epochs\": 10,\n",
        "    \"save_dir\": \"logs/\",\n",
        "}\n",
        "\n",
        "# from argparse import (Namespace)\n",
        "# hparams = Namespace(**hparams)\n",
        "\n",
        "# import json\n",
        "# import types\n",
        "# def load_object(dct):\n",
        "#     return types.SimpleNamespace(**dct)\n",
        "# hparams = json.loads(json.dumps(hparams), object_hook=load_object)\n",
        "\n",
        "experiment_name = \"celeba_\" + str(random.randint(1e4, 1e5))\n"
      ],
      "metadata": {
        "id": "3QNol8jnzFTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datamodule = CelebADataModule(**hparams[\"dataset\"])\n",
        "\n",
        "model = ResNet(hparams)\n",
        "\n",
        "ckpt_callback = pl.callbacks.ModelCheckpoint(\n",
        "    dirpath=os.path.join(hparams[\"save_dir\"], experiment_name),\n",
        "    filename=\"checkpoint-{epoch:03d}-{val/epoch:avg_acc:.3f}-{val/epoch:wga:.3f}\",\n",
        "    monitor=\"val/epoch:wga\",\n",
        "    save_last=True,\n",
        "    save_top_k=1,\n",
        "    mode=\"max\",\n",
        "    auto_insert_metric_name=False,\n",
        ")\n",
        "\n",
        "progress_bar = pl.callbacks.progress.TQDMProgressBar(refresh_rate=1)\n",
        "lr_callback = pl.callbacks.LearningRateMonitor(logging_interval=\"step\")\n",
        "logger = pl.loggers.TensorBoardLogger(save_dir=f\"logs/{experiment_name}/\")\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "accelerator = \"cuda\" if use_cuda else \"cpu\"\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    accelerator=accelerator,\n",
        "    devices=\"auto\",\n",
        "    benchmark=True,\n",
        "    enable_progress_bar=True,\n",
        "    log_every_n_steps=1,\n",
        "    num_sanity_val_steps=1,\n",
        "    check_val_every_n_epoch=1,\n",
        "    max_epochs=hparams[\"max_epochs\"],\n",
        "    callbacks=[ckpt_callback, progress_bar, lr_callback],\n",
        "    logger=logger,\n",
        ")\n",
        "trainer.fit(\n",
        "    model=model,\n",
        "    datamodule=datamodule,\n",
        "    ckpt_path=None,\n",
        ")\n"
      ],
      "metadata": {
        "id": "1MoN2Ch-wB84"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}